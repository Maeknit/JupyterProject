{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Кодирование текста в качестве мешка слов\n",
    "Даны текстовые данные, и требуется создать набор признаков, <br>\n",
    "указывающих на количество вхождений определенного слова в текст наблюдения."
   ],
   "id": "594afc1fc683bc69"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T10:01:35.366726Z",
     "start_time": "2025-10-14T10:01:34.191524Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['Бразилия - моя любовь. Бразилия!',\n",
    "                      'Швеция - лучше',\n",
    "                      'Германия бьет обоих'])\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "bag_of_words"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 8 stored elements and shape (3, 8)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:02:01.278198Z",
     "start_time": "2025-10-14T10:02:01.272495Z"
    }
   },
   "cell_type": "code",
   "source": "bag_of_words.toarray()",
   "id": "a375e43ba83b5f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:02:39.022983Z",
     "start_time": "2025-10-14T10:02:39.017646Z"
    }
   },
   "cell_type": "code",
   "source": "count.get_feature_names_out()",
   "id": "8a991c7013588bb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['бразилия', 'бьет', 'германия', 'лучше', 'любовь', 'моя', 'обоих',\n",
       "       'швеция'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Одним из наиболее распространенных методов преобразования текста в признаки является использование модели мешка слов.<br>\n",
    "Эти модели выводят признак для каждого уникального слова в текстовых данных, при этом каждый признак содержит кол-во вхождений в наблюдениях.<br><br>\n",
    "Большинство слов, не встречаются в большинстве наблюдений, и поэтому матрицы признаков на основе мешка слов будут в качестве значений содержать в основном нули. Эти матрицы называются ***разреженного*** типа. Одной из особенностей векторизатора частностей CountVectorizer <br>\n",
    " <br>\n",
    " Каждый признак является словом - это необязательно. Вместо этого мы можем установить, чтобы каждый признак был комбинацией <br>\n",
    " двух слов (2-граммами) или трех слов (3-граммами). Параметр ngram_range устанавливает минимальный и максимальный размеры наших n-gram. <br>\n",
    " Так же можно удалить слова с низкой информацией, используя стоп-слова в параметре stop_words либо с помощью встроенного списка, либо с помощью собственного списка"
   ],
   "id": "5ccd95c9383f267b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:56:04.286758Z",
     "start_time": "2025-10-14T10:56:04.280639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_2gram = CountVectorizer(ngram_range=(1, 2),\n",
    "                              stop_words='english',\n",
    "                              vocabulary=['бразилия'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "bag.toarray()"
   ],
   "id": "5a6049e2091b0bdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Взвешивание важности слов",
   "id": "3ebe0c2212bbda86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Требуется мешок слов, но со словами, взвешенными по их важности для наблюдения.<br>\n",
    "Решение: сравнить частоту слова в документе, используя - обратную документную частоту (tf-idf)."
   ],
   "id": "ddd8bdf559b93b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T11:33:12.323836Z",
     "start_time": "2025-10-14T11:33:12.316064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['Бразилия - моя любовь. Бразилия!',\n",
    "                      'Швеция - лучше',\n",
    "                      'Германия бьет обоих'])\n",
    "vectorizer = TfidfVectorizer() # приводит токены к нижнему регистру  и удаляет пунктуацию по умолчанию\n",
    "tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "tfidf_matrix.toarray()"
   ],
   "id": "85a027b959a2dc36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81649658, 0.        , 0.        , 0.        , 0.40824829,\n",
       "        0.40824829, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.70710678],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.57735027, 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T11:33:12.687299Z",
     "start_time": "2025-10-14T11:33:12.682434Z"
    }
   },
   "cell_type": "code",
   "source": "vectorizer.vocabulary_",
   "id": "3d2a64dcf35d2d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бразилия': 0,\n",
       " 'моя': 5,\n",
       " 'любовь': 4,\n",
       " 'швеция': 7,\n",
       " 'лучше': 3,\n",
       " 'германия': 2,\n",
       " 'бьет': 1,\n",
       " 'обоих': 6}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Чем чаще слово попадается в документе, тем более вероятно, что оно важно для этого документа.<br>\n",
    "Эта мера частоты встречаемости слова в документе называется словарной частотой (tf).<br><br>\n",
    "А если слово встречается во многих документах, но менее важно для любого отдельного документа, <br>\n",
    "то эта мера называется документной частотой (df).<br><br>\n",
    "Объединив эти два статистических показателя, мы можем назначить оценку каждому слову, тем самым показывая, насколько важно это слово:\n",
    "$$\n",
    "tf-idf(t,d)=tf(t,d)\\times idf(t,d)\n",
    "$$\n",
    "где *t* - слово; *d* - документ.<br>\n",
    "tf - это просто количество раз, когда слово появляется в документе, и idf рассчитывается следующим образом:\n",
    "$$\n",
    "idf(t)= \\log{\\frac{1+n_d}{1+df(d,t)}}+1\n",
    "$$\n",
    "где n_d - это количество документов; df(d,t) - документная частота слова t"
   ],
   "id": "ec325b1813c7ba9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
